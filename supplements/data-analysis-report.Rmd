---
title: "Sampling Strategies in DfE"
subtitle: "Data Analysis Report"
author: "Linus Hof [![ORCID iD](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-2257-2136)"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: true
    keep_md: true
---

```{r setup, include = FALSE}
# set global chunk options
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "c", 
                      fig.height = 8, 
                      fig.width = 12)
```


```{r pkgs}
pacman::p_load(tidyverse, viridis, ggpubr, latex2exp, stringr, scico, papaja)
```

```{r data}
simulation_roundwise <- read_rds("C:/Users/ge84jux/Projects/sampling-strategies/data/simulation_roundwise.rds.bz2")
simulation_roundwise <- simulation_roundwise %>% mutate(psi = 1-(psi+.5))
choice_data <- read_rds("C:/Users/ge84jux/Projects/sampling-strategies/data/choice_data.rds.bz2")
cpt <- read_rds("C:/Users/ge84jux/Projects/sampling-strategies/data/cpt_estimates.rds")
```

# Description

...

# Core Results and Message

...

# Sampling Behavior

In this section, I look at how the roundwise integration model (RIM) and the summary integration model (SIM) affect the sampling process.
Specifically, I show how the interplay of the models and parameter combinations - i.e., the switching probability $\psi$, the threshold $\theta$, and the threshold type (absolute vs. relative) - impact the number of outcomes sampled prior a final choice.
I further look at whether and how the sampled relative frequencies of outcomes deviate from their objective probabilities.

## Number of sampled outcomes

I start by plotting the total number of outcomes sampled prior each final choice.
The following figure displays the median for each model and parameter combination:

```{r labeller functions, include=FALSE}
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_psi <- function(string) {
  TeX(paste("$\\psi=$", string, sep = "")) #switching probability parameter psi
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}


```

```{r total sample sizes}
trial_n_median <- choice_data %>% 
  group_by(model, psi, threshold, theta) %>% 
  summarise(med = median(n_sample))

trial_n_median %>% 
  ggplot(aes(x = psi, y = med, color = threshold)) + 
  facet_wrap(~model~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = expression(paste("Switching probability ", psi)),
       y = "Median sample size", 
       color = "Threshold") +
  geom_hline(yintercept = 14, linetype = "dashed", color = "white") + # meta-analytic median for choices between a safe and a risky prospect
  geom_point(size = 2) + 
  geom_line() + 
  ggtitle("Total sample size prior final choice") +
  theme_apa()
```

### Roundwise integration model

* For the RIM, as expected, **increases in the switching probability cause decreases in the total sample size**.
The following figure demonstrates that this inverse relationship is driven by the effect of the switching probability on the number of sampled outcomes encompassed in each comparison round.
I.e., **high switching probabilities cause each (mean) comparison round to be based on a small number of samples and vice versa**.

```{r round sample sizes}
n_round <- simulation_roundwise %>% 
  group_by(psi, threshold, theta, problem, agent, round) %>% 
  summarise(n_round = n())

n_round_median <- n_round %>% 
  group_by(psi, threshold, theta) %>% 
  summarise(med = median(n_round))

n_round_median %>% 
  ggplot(aes(psi, med, color = threshold)) +
  facet_wrap(~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(paste("Switching probability ", psi)),
       y = "Median sample size", 
       color = "Threshold") +
  geom_point(size = 2) + 
  geom_line() +
  ggtitle("Median sample size within a comparison round") + 
  theme_apa()
```

* An increase in the threshold - i.e., the required number of won comparisons - leads to a **proportional increase in the total sample size**.

* The median total sample sizes show that switching probabilities and thresholds elsewhere assumed to relate closely to the roundwise strategy - i.e., high switching probabilities and high thresholds - lead to **total sample sizes that approximate the meta-analytic mean found by Wulff et al. (2018)** (see dashed line). 
In contrast, low (high) switching probabilities and large (small) thresholds produce total sample sizes that are considerably larger (smaller) than than the meta-analytic median.
Assuming that people make systematic choices - i.e., choices that are based on true differences in the latent properties of choice alternatives - then whatever property is used to assess differences between the options may be accurately inferred at a total sample size of about 14. 
One might reverse-engineer what property that is depending on the model and parameter combinations.

### Summary integration model

* For the SIM and relative thresholds, increases in the switching probability cause increases in the total sample size.
However, this effect is caused by **random temporal imbalances between options for small switching probabilities** that are introduced by the method of implementation (i.e., the use of cumulative sums instead of means).
Specifically, the evidence accumulation process starts at random on one of the options.
This option obtains a temporal advantage over the other option because the accumulation of evidence starts sooner. 
The smaller the switching probability, the larger the temporal advantage.
Vice versa, the higher the switching probability, the smaller the temporal advantage.
These **random temporal imbalances must be taken into account when interpreting the choice data and CPT estimates**

* The **temporal imbalances have severe effects on the number of sampled outcomes for relative thresholds** but not so much for absolute thresholds.
That is, for absolute thresholds, the number of outcomes that must be sampled from a given option in order to reach a threshold is not affected by the switching probability. 
In contrast, the lower the switching probability and the larger the temporal imbalances between options, the less outcomes must be sampled from the advantageous option for it to reach the relative threshold. 

* For both absolute and relative thresholds, increases in the threshold naturally lead to proportional increases in the total number of samples.

## Objective probabilities vs. sampled relative frequencies

Below, it is shown how the sampled relative frequencies of outcomes match the objective probabilities.
A deviation of the sampled from the latent (objective) information is commonly used to explain the *as-if* underweighting of rare outcomes pattern in decisions from experience. 
That is, in samples of small size, outcomes of small probability tend to be sample less rather than more often than objectively warranted (undersampling or underrepresentation).
The relation between objective probabilities and sampled relative frequencies is displayed separately for each model and threshold type.  

```{r median relative frequencies}
ep_median_low <- choice_data %>% 
  group_by(model, psi, threshold, theta, p_r_low) %>% 
  summarise(median = median(ep_r_low, na.rm = TRUE))
```

### Roundwise

```{r median relative frequencies RIM absolute}

ep_median_low_ra <- ep_median_low %>% filter(model == "roundwise" & threshold == "absolute")

choice_data %>% 
  filter(model == "roundwise" & threshold == "absolute") %>%
  ggplot(aes(x = p_r_low, y = ep_r_low)) + 
  facet_grid(psi~theta, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed), psi = as_labeller(label_psi, default = label_parsed))) +
  scale_x_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  scale_y_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  geom_density_2d_filled(contour_var = "ndensity") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed", size = .7) + 
  geom_point(data = ep_median_low_ra, aes(y  = median), color = "white", size = .5) + 
  labs(x = "Objective probability of low-ranked outcome",
       y = "Sampled relative frequency of low-ranked outcome", 
       title = "Density and median of sampled relative frequencies | RIM absolute threshold") +
  theme_apa()
```

```{r median relative frequencies RIM relative}

ep_median_low_rr <- ep_median_low %>% filter(model == "roundwise" & threshold == "relative")

choice_data %>% 
  filter(model == "roundwise" & threshold == "relative") %>%
  ggplot(aes(x = p_r_low, y = ep_r_low)) + 
  facet_grid(psi~theta, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed), psi = as_labeller(label_psi, default = label_parsed))) +
  scale_x_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  scale_y_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  geom_density_2d_filled(contour_var = "ndensity") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed", size = .7) + 
  geom_point(data = ep_median_low_rr, aes(y  = median), color = "white", size = .5) + 
  labs(x = "Objective probability of low-ranked outcome",
       y = "Sampled relative frequency of low-ranked outcome", 
       title = "Density and median of sampled relative frequencies | RIM relative threshold") +
  theme_apa()
```

* *Threshold level*: As expected, the **sampled relative frequencies match the objective probabilities more closely the higher the thresholds are** (horizontal grid dimension, from left to right). 
That is, the higher the thresholds, the more outcomes are sampled, the more closely the sampled relative frequencies converge to the objective relative frequencies.
**For low thresholds, the majority of sampled relative frequencies tends to underrepresent small-probability outcomes** and overrepresent high-probability outcomes. 
This is expected from the **positive skew of the binomial distribution** of small-probability outcomes.

* *Switching probability*: **For low switching probabilities, sampled relative frequencies tend to match the objective probabilities more closely than for high switching probabilities** (vertical grid dimension, from top to bottom). 
This is because the total number of sampled outcomes increases with decreasing switching probabilities.
Again, because of the positive skew of their binomial distribution, small-probability outcomes then tend to be underrepresented rather than overrepresented the majority of times.

### Summary 

```{r median relative frequencies SIM absolute}
ep_median_low_sa <- ep_median_low %>% filter(model == "summary" & threshold == "absolute")

choice_data %>% 
  filter(model == "summary" & threshold == "absolute") %>%
  ggplot(aes(x = p_r_low, y = ep_r_low)) + 
  facet_grid(psi~theta, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed), psi = as_labeller(label_psi, default = label_parsed))) + 
  scale_x_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  scale_y_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  geom_density_2d_filled(contour_var = "ndensity") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed", size = .7) + 
  geom_point(data = ep_median_low_sa, aes(y  = median), color = "white", size = .5) + 
  labs(x = "Objective probability of low-ranked outcome",
       y = "Sampled relative frequency of low-ranked outcome", 
       title = "Density and median of sampled relative frequencies | SIM absolute threshold") +
  theme_apa()
```

```{r median relative frequencies SIM relative}
ep_median_low_sr <- ep_median_low %>% filter(model == "summary" & threshold == "relative")

choice_data %>% 
  filter(model == "summary" & threshold == "relative") %>%
  ggplot(aes(x = p_r_low, y = ep_r_low)) + 
  facet_grid(psi~theta, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed), psi = as_labeller(label_psi, default = label_parsed))) + 
  scale_x_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  scale_y_continuous(limits = c(-.1, 1.1), breaks = seq(0, 1, .5)) + 
  geom_density_2d_filled(contour_var = "ndensity") + 
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed", size = .7) + 
  geom_point(data = ep_median_low_sr, aes(y  = median), color = "white", size = .5) + 
  labs(x = "Objective probability of low-ranked outcome",
       y = "Sampled relative frequency of low-ranked outcome", 
       title = "Density and median of sampled relative frequencies | SIM relative threshold") +
  theme_apa()
```

* *Threshold level*: As expected from the laws of large numbers and similar to the roundwise integration model, the sampled relative frequencies match the objective probabilities more closely the higher the thresholds are. 
That is, the higher the thresholds, the more outcomes are sampled, the more closely the sampled relative frequencies converge to the objective relative frequencies.

* *Switching probability*: For absolute thresholds, switching probabilities have a rather negligible effect on the relative sampled frequencies.
This can be explained by the fact, that for absolute thresholds changes in the switching probability have no effect on the total number of sampled outcomes.
However, for relative thresholds, a pattern to that of the roundwise integration model emerges.
That is, **for high (rather than low) switching probabilities, sampled relative frequencies tend to match the objective probabilities more closely**. 
This is because the total number of sampled outcomes increases with increasing switching probabilities.
Again, because of the positive skew of their binomial distribution, small-probability outcomes then tend to be underrepresented rather than overrepresented the majority of times.

# Choice Behavior

In this section, I show how the interplay between the models and the parameter combinations shape the final choice. 
The models entail different mechanisms for how the sampled outcomes are integrated and evaluated.
The parameter combinations govern how (and how many) outcomes are sampled in the first place.
Thus, the choice patterns below show how **sampling strategies and integration strategies may act together to shape the final choice in DfE**.
However, to inquire whether the interplay between sampling and integration strategies exercises an effect on the choice behavior - more concretely, on deviations from expected value maximization - beyond the well-known effect of small samples, I take the deviation of the sampled relative frequencies from the objective probabilities into account.
That is, the choice patterns are displayed as both deviations from the sampled mean and from the latent expected value. 

I start by plotting the rates of choices that did not maximize the EV and sampled mean (false response rates), separately for models, parameter combinations and the existence and rank (desirability) of a small-probability outcome. 

```{r choice-rates}

#prepare data

## omit trials where only one option was attended 
choice_data <- choice_data %>% 
  filter(!c(n_s == 0 | n_r == 0))

## determine normative choice according to latent EV and sampled mean

### latent EV

fr_rates_ev <- choice_data %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "r", ev_ratio < 1 ~ "s")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(model, psi, threshold, theta, rare, norm, choice) %>% # separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "r" & choice == "s" ~ "Safe", #determine false response type
                          norm == "s" & choice == "r" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         norm = "EV")

### sampled mean

fr_rates_mean <- choice_data %>%
  mutate(norm = case_when(mean_r/safe > 1 ~ "r", mean_r/safe < 1 ~ "s")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(model, psi, threshold, theta, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "r" & choice == "s" ~ "Safe", #determine false response type
                          norm == "s" & choice == "r" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         norm = "Mean")

fr_rates <- bind_rows(fr_rates_ev, fr_rates_mean)
```

## Roundwise integration model

```{r choice-rates RIM absolute}

fr_rates_ra_risky <- fr_rates %>% filter(model == "roundwise" & threshold == "absolute" & type == "Risky")
fr_rates_ra_safe <- fr_rates %>% filter(model == "roundwise" & threshold == "absolute" & type == "Safe")

fr_rates_ra_risky %>%
  ggplot(aes(psi, rate, color = norm)) +
  facet_grid(rare~theta, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed),
                                             theta = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  labs(x = expression(paste("Switching Probability ", psi)),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse",
       title = "False response rates | RIM absolute thresholds") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_ra_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_ra_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#919191", "#9EB0FF")) + 
  theme_minimal()
```

```{r choice-rates RIM relative}

fr_rates_rr_risky <- fr_rates %>% filter(model == "roundwise" & threshold == "relative" & type == "Risky")
fr_rates_rr_safe <- fr_rates %>% filter(model == "roundwise" & threshold == "relative" & type == "Safe")

fr_rates_rr_risky %>%
  ggplot(aes(psi, rate, color = norm)) +
  facet_grid(rare~theta, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed),
                                             theta = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(paste("Switching Probability ", psi)),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse",
       title = "False response rates | RIM relative thresholds") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_rr_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_rr_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#919191", "#9EB0FF")) + 
  theme_minimal()
```


* The roundwise integration model behaves similar for absolute and relative thresholds.
For small-probability outcomes of high rank, the rates of false safe choices increase with the switching probability, whereas the rates of false risky choices remain low.
Vice versa, for small-probability outcomes of low rank, the rate of false risky choices increases with the switching probability, whereas the rates of false safe choices remain low. 
Both patterns, similar in structure, can be explained by the inverse relationship between the switching probability and sample size and the positive skew of of small-probability outcomes. 
That is, in small samples, small-probability outcomes tend to be undersampled, causing the mean of the risky prospect to be inflated (deflated) if the low (high) rank outcome is of small probability. 
In turn, for inflated (deflated) means, there is a higher chance that the risky prospect is falsely chosen (dismissed) and this chance increases with the switching probability.

* The effect of total sampling error is represented by the divergence of the colored (mean) line from the gray (EV) line:
False response rates tend to be higher if compared to the latent EV instead of the sampled mean, where the latter controls for sampling error.
The sampling error causes the sampled relative frequencies to deviate from the objective probabilities both unsystematically (random random) and systematically (systematic underrepresenation of small probability outcomes).
With increasing thresholds, both error types are reduced as reflected by the convergence of the two lines. 
I.e., the higher the total number of sampled outcomes, the more does the sampled mean converge against the expected value.
The similarity of the curve-pairs demonstrates that the interplay between sampling and integration strategies can have a nuanced and severe effect on the choice behavior beyond sampling error. 

* For choice problems with no small-probability outcomes, false response rates increase with switching probabilities, however, on a low level.
Together with the reversed choice patterns for problems that contain desirable rare outcomes on the one hand and undesirable rare outcomes on the other hand, this shows that the structure of the environment (choice problems) is an important factor.

## Summary integration model

```{r choice-rates SIM absolute}

fr_rates_sa_risky <- fr_rates %>% filter(model == "summary" & threshold == "absolute" & type == "Risky")
fr_rates_sa_safe <- fr_rates %>% filter(model == "summary" & threshold == "absolute" & type == "Safe")

fr_rates_sa_risky %>%
  ggplot(aes(psi, rate, color = norm)) +
  facet_grid(rare~theta, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed),
                                             theta = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(paste("Switching Probability ", psi)),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse",
       title = "False response rates | SIM absolute thresholds") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_sa_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_sa_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#919191","#FFACAC")) + 
  theme_minimal()
```


```{r choice-rates SIM relative}

fr_rates_sr_risky <- fr_rates %>% filter(model == "summary" & threshold == "relative" & type == "Risky")
fr_rates_sr_safe <- fr_rates %>% filter(model == "summary" & threshold == "relative" & type == "Safe")

fr_rates_sr_risky %>%
  ggplot(aes(psi, rate, color = norm)) +
  facet_grid(rare~theta, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed),
                                             theta = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(paste("Switching Probability ", psi)),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse",
       title = "False response rates | SIM relative thresholds") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_sr_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_sr_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#919191", "#FFACAC")) + 
  theme_minimal()
```

* In contrast to the roundwise integration model, false response rates decrease with increasing switching probabilities.
Specifically, the false response rates are transitioning from random choice behavior at rates around .5 for low switching probabilities to a systematic maximization of the sampled mean (EV) for high switching probabilities. 
This demonstrates that changes in the sampling strategy do not cause the summary integration model to make systematically different choices.
That is, the mechanisms of the summary integration model are such that evidence is accumulated for differences in the latent EVs, given that approximately the same number of outcomes is sampled from *both* prospects.
The more outcomes are sampled from both prospects, i.e., the higher the thresholds, the more accurately these differences should be assessed. 
Changes on the sampling strategy do not change the latent property for which evidence is accumulated in the summary integration model.
Rather, they affect whether approximately the same number of outcomes is sampled from both prospects or whether one of the prospects, obtains, by chance, a temporal advantage, rendering the decision either systematic or unsystematic, respectively. 

* For small switching probabilities, the effect of sampling error should be overshadowed by the temporal imbalances. 
That is, as no systematic choices are made, deviations from the sampled mean and the latent expected value should be equally random. 
As the switching probability increases, false response rates should be larger for the latent EV than for the sampled mean.
However, as the number of sampled outcomes increases with the switching probabilities, the effect of systematic sampling error is generally low.  


# Stochastic Cumulative Prospect Theory (CPT)

In the following, I analyse whether and how the choice patterns translate to the parameters of cumulative prospect theory's value and weighting function.
Critically, these functions offer an description for how sensitive choices are to changes in the probability and outcome magnitude.
Because I am interested in the effects of sampling and integration strategies beyond total sampling error, the sampled relative frequencies are supplied as probability information to CPT's weighting function.

## Choice Sensitivity

I start by plotting the estimates for the choice sensitivity parameter $\rho \geq 0$, which indicates how sensible the model-implied choices are regarding the evaluations of the core model.

```{r}

# rho estimates

rho <- cpt %>%
  filter(parameter == "rho", threshold == "absolute") %>%
  ggplot(aes(psi, mean, color = model)) + 
  facet_wrap(~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1.1), breaks = seq(0,1,.5)) + 
  scale_y_continuous(limits = c(0,5), breaks = seq(0,5, 2.5)) + 
  labs(x = expression(paste("Switching probability", psi)), 
       y = expression(paste("Choice sensitivity ", rho)),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  scale_color_scico_d(palette = "berlin") + 
  theme_minimal()
rho
```

* For the **roundwise integration model**, all estimates of $rho$ take values $> 1$, indicating that the choices are not random.
However, the estimates decrease with increasing switching probabilities, indicating a diminishing degree of consistency of the model-implied choices with the evaluations - i.e., the strength of preferences - derived from the core CPT model.
*This may indicate that the degree to which the assumptions of the descriptive CPT model fit the assumptions of the generative roundwise model decreases with increasing switching probabilities*. 

```{r include = FALSE}
wf <- tibble(p = seq(0, 1, .01)) %>%  #cumulative probabilities
  expand_grid(gamma = seq(.1, 10, .1), #gamma values
              delta = c(.1, .5, 1, 2, 5, 10)) %>% #delta values
  mutate(wp = (delta*(p^gamma))/((delta*p^gamma)+(1-p)^gamma)) #images of wf

#labeller function for facet labels with LateX math expressions 
label_delta <- function(string) {
  TeX(paste("$\\delta=$", string, sep = ""))  
}

#plot shapes of weighting function
wf %>% 
  ggplot(aes(p, wp, group = gamma)) +
  facet_wrap(~delta, labeller = as_labeller(label_delta, default = label_parsed)) + 
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) + 
  labs(x = "Probability of High-Rank Risky Outcome",
       y = "Transformed Probability (Decision Weight)", 
       color = expression(gamma)) + 
  theme_dark() + 
  geom_line(aes(color = gamma), size = .5) +
  scale_color_scico(palette = "tokyo") + 
  geom_abline(intercept = 0, slope = 1, color = "gray", linetype = "dashed")
```


* For the **summary integration model**, the estimates of $\rho$ increase with switching probabilities. 
Because $\rho = 0$ indicates that the model-implied choices are insensitive to the evaluations of the core CPT model and thus, random, the estimates fit well with the finding of random temporal imbalances that govern the choices of the summary integration for low switching probabilities.  

## Weighing function

Below, the parameter estimates the weighting function are plotted. 

```{r fig.height= 12}

# gamma estimates

gamma <- cpt %>%
  filter(parameter == "gamma", threshold == "absolute") %>%
  ggplot(aes(psi, mean, color = model)) + 
  facet_wrap(~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1.1), breaks = seq(0,1,.5)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0,2, 1)) + 
  labs(x = element_blank(), 
       y = expression(paste("Curvature ", gamma)),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed") + 
  scale_color_scico_d(palette = "berlin") + 
  theme_minimal()

# delta estimates

delta <- cpt %>%
  filter(parameter == "delta", threshold == "absolute") %>%
  ggplot(aes(psi, mean, color = model)) +
  facet_wrap(~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1.1), breaks = seq(0,1,.5)) + 
  scale_y_continuous(limits = c(0,10), breaks = seq(0,10,5)) + 
  labs(x = element_blank(), 
       y = expression(paste("Elevation ", delta)),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_scico_d(palette = "berlin") + 
  geom_hline(yintercept = 1, linetype = "dashed") + 
  theme_minimal()


wf_parameters <-  ggarrange(gamma, delta, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO")
wf_parameters
```

### Roundwise integration model

* For ($\theta = 1$, $\psi = 1$), the 95%-intervals of the posterior distributions are large.
Here, no relative frequencies other than 0 and 1 are supplied to the weighting function, which explains that the values in between cannot be reliably accounted for.

* For ($\theta = 1$, $\psi < 1$), the estimates imply a linear weighting pattern, i.e., ($\gamma \approx 1, \delta \approx1$). 
This is because only a single mean comparison is carried, therefore causing roundwise integration strategy to always choose the prospect that produces the larger mean.
This result is well in line with the explanation that if the mind were to infer the latent objective probabilities of outcomes from the sampled relative frequencies and to follow the principle of EV maximization, sampling error can account for any deviations from it.
This is also demonstrated by the false response rates displayed above, where there are no choices for $\theta = 1$ that do not maximize the sampled mean.

* For $\theta > 1$, the curvature parameter $\gamma$ takes values $\geq 1$ which increase with switching probabilities, resulting in a increasingly pronounced S-shaped weighting function (see below).
In other words, the higher the switching probability, the more severe is the underweighting (overweighting) of the high-rank outcome of the risky prospect in CPT, if it is of small (large) probability.
That is, the as-if underweighting of rare outcomes pattern indicated by the final choices (i.e., the false response rates) translates rather directly to an underweighting of small-probability outcomes in CPT.

* The elevation parameter $\delta$ takes values $> 1$, however, the estimates increase slightly with switching probabilities, causing the overweighting of the high-rank outcome of the risky prospect to extend across the mid-point of the probability scale.
*It is open for discussion whether this extension of overweighting beyond the mid-point is theoretically implied by the mechanisms of the round-wise integration model: That is, for high switching probabilities, the model is expected to choose the risky prospect, if the probability of its high-rank outcome is $> .5$. To account for the implied choice behavior, CPT might have to overweight high-rank outcomes with a probability far larger than .5 less severely than high-rank outcomes with a probability only a little larger than .5. Such a pattern is implied for $\delta > 1$.*

* Leaving aside this open issue, the estimates for the $\gamma$ and $\delta$ parameter show that the choices implied by the potential interplay of different sampling strategies and the round-wise integration/decision strategy can lead to distinct signatures in CPT's weighting function (see Figure\ \@ref(fig:function-forms), for the resulting graphs of the weighting function).
Once again, these signatures cannot be due to sampling error.

### Summary integration model

* The estimates for the choice sensitivity parameter $rho$ tend to be low for small switching probabilities, implying that in these cases the highest model fit is achieved, if a random choice is assumed.
Thus, for small switching probabilities, interpretations of the parameter estimates for the core CPT model should be made with great caution (if at all).

* For high switching probabilities, the estimates for the weighting function's $\gamma$ and $\delta$ parameter imply a linear weighting of sampled relative frequencies, i.e., $\gamma \approx 1$ and $\delta \approx 1$.

## Value function

```{r fig.height=12}

# alpha estimates

alpha <- cpt %>%
  filter(parameter == "alpha", threshold == "absolute") %>%
  ggplot(aes(psi, mean, color = model)) +
  facet_wrap(~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1.1), breaks = seq(0,1,.5)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
  labs(x = expression(paste("Switching Probability ", psi)), 
       y = expression(paste("Concavity ", alpha)),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_scico_d(palette = "berlin") + 
  geom_hline(yintercept = 1, linetype = "dashed") + 
  theme_minimal()

alpha
```

## Graphs

```{r fig.height = 12}

# weighting function

weights <- cpt %>%
  select(model, psi, threshold, theta, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

wf <- weights %>% 
  filter(threshold == "absolute") %>% 
  ggplot(aes(p, w, group = psi, color = psi)) +
  facet_wrap(~model~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") + 
  labs(x = "Sampled relative frequency",
       y = expression(paste("Decision weight ", pi)),
       color = expression(paste("Switching\nProbability ", psi))) +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .7) +
  scale_color_scico(palette = "acton", end = .8) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  theme_minimal() 
  
# value function

values <- cpt %>%
  select(model, psi, threshold, theta, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

vf <- values %>% 
  filter(threshold == "absolute") %>% 
  ggplot(aes(x, v, group = psi, color = psi)) +
  facet_wrap(~model~theta, nrow = 2, labeller = labeller(theta = as_labeller(label_theta, default = label_parsed)), scales = "free") + 
  labs(x = "Objective outcome",
       y = "Subjective value",
       color= expression(paste("Switching Probability ", psi))) +
  scale_x_continuous(limits = c(0,20), breaks = seq(0, 20, 10)) +
  scale_y_continuous(limits = c(0,20), breaks = seq(0,20,10)) + 
  geom_line(size = .7) +
  scale_color_scico(palette = "acton", end = .8) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  theme_minimal()

# merge plots

cpt_summary <- ggarrange(wf, vf, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO")
cpt_summary
```


